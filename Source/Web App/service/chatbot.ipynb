{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-03T08:38:39.259279Z",
     "iopub.status.busy": "2025-01-03T08:38:39.259019Z",
     "iopub.status.idle": "2025-01-03T08:41:52.279478Z",
     "shell.execute_reply": "2025-01-03T08:41:52.278443Z",
     "shell.execute_reply.started": "2025-01-03T08:38:39.259252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "\n",
    "!pip install llama-index\n",
    "!pip install sentence-transformers\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-readers-json\n",
    "!pip install llama-index-vector-stores-milvus\n",
    "!pip install pymilvus\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install llama-index-llms-groq\n",
    "!pip install FlagEmbedding\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install loralib\n",
    "!pip install einops\n",
    "!pip install huggingface_hub\n",
    "!pip install python-dotenv\n",
    "\n",
    "!pip install flask-ngrok\n",
    "!pip install pyngrok\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:41:52.281303Z",
     "iopub.status.busy": "2025-01-03T08:41:52.280993Z",
     "iopub.status.idle": "2025-01-03T08:47:46.273291Z",
     "shell.execute_reply": "2025-01-03T08:47:46.272426Z",
     "shell.execute_reply.started": "2025-01-03T08:41:52.281274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36148793fff74d54958c1125b39f7a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/796 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687889a8bb9443eebb4f7a270a36096c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2656e0a0afd54c6db99c6b9ff1384e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c6951240344adb2a6f9a7713d53d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a138d1a30d05461ab235951d10396f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24d3a5428284b03a8fe84a8e2786bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b421aab47884e1695600fabc0049080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2fbad6cf0a42de9699fedf4b8a4be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44855660fb054d808e9bbbdaf4ac6780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c47badb3b974bc282908c230f267fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42508503d13e4059bf23fe80d80d5465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c9159e4f8b48b9907b790c2982c84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b940aee0d5410797726201e1a78426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7682f90b14c486581ac54249ab31a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/322 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model configurations\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "\n",
    "REPO_ACCESS_TOKEN = os.getenv(\"REPO_ACCESS_TOKEN\")\n",
    "MODEL_NAME = \"Namronaldo2004/math-vinallama-7b-chat\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    "    quantization_config = bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "model.generation_config = generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:47:46.276431Z",
     "iopub.status.busy": "2025-01-03T08:47:46.275586Z",
     "iopub.status.idle": "2025-01-03T08:47:46.284768Z",
     "shell.execute_reply": "2025-01-03T08:47:46.283905Z",
     "shell.execute_reply.started": "2025-01-03T08:47:46.276390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Chatbot\n",
    "def generate_response(query: str, history: dict[str, str]):\n",
    "    prompt = f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Bạn là một chuyên gia về toán. Khi nhận được yêu cầu từ người dùng, dựa vào những kiến thức mà bạn đã có, hãy trả lời người dùng một cách nhất quán, đầy đủ nhưng tránh dư thừa thông tin, và hơn hết nội dung câu trả lời phải chính xác nhất\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    ### Yêu cầu:\n",
    "    {query}\n",
    "    ### Câu trả lời:\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\".strip()\n",
    "\n",
    "    init_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Bạn là một chuyên gia về toán. Khi nhận được yêu cầu từ người dùng, dựa vào những kiến thức mà bạn đã có, hãy trả lời người dùng một cách nhất quán, đầy đủ nhưng tránh dư thừa thông tin, và hơn hết nội dung câu trả lời phải chính xác nhất\"\n",
    "    }\n",
    "        \n",
    "    history = [init_msg, *history, {\"role\": \"user\", \"content\": query}]\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    encoding = tokenizer(prompt, return_tensors = \"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids = encoding.input_ids,\n",
    "            attention_mask = encoding.attention_mask,\n",
    "            generation_config = generation_config\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens = True).split(\"<|im_start|> assistant\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:47:46.286039Z",
     "iopub.status.busy": "2025-01-03T08:47:46.285780Z",
     "iopub.status.idle": "2025-01-03T08:47:49.049626Z",
     "shell.execute_reply": "2025-01-03T08:47:49.048673Z",
     "shell.execute_reply.started": "2025-01-03T08:47:46.286014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n",
      "Service is live at: NgrokTunnel: \"https://69f1-34-44-191-114.ngrok-free.app\" -> \"http://localhost:5000\"\n"
     ]
    }
   ],
   "source": [
    "# Expose api with ngrok\n",
    "\n",
    "!ngrok config add-authtoken 2r1MdG5rU3fQL4aoO92QFztrerN_ejwgH3dUcde6eiEELLgC\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# Create public URL for the app\n",
    "public_url = ngrok.connect(5000)\n",
    "print(\"Service is live at:\", public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-03T09:15:48.694Z",
     "iopub.execute_input": "2025-01-03T08:47:49.051242Z",
     "iopub.status.busy": "2025-01-03T08:47:49.050880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "# Flask app\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return jsonify({ \"message\": \"Service is alive\" }), 200\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "  data = request.get_json()\n",
    "  reqMsg = data.get(\"message\", None)\n",
    "  reqHistory = data.get(\"history\", None)\n",
    "  if not reqMsg or reqHistory == None: return jsonify({ \"message\": \"Bad request\"}), 400\n",
    "  try:\n",
    "    resMsg = generate_response(reqMsg, reqHistory)\n",
    "    return jsonify({ \"message\": resMsg }), 200\n",
    "  except:\n",
    "    return jsonify({ \"message\": \"Server internal error\"}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
